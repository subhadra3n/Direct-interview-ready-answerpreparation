1.What type of incidents we manage.
2.what are your daily tasks
3.


_______________________________________________________________
storing telling answer, below one gives if incident occurs how we solve them:

1.what type of incidents we manage?
We manage incidents across infrastructure, application, deployment, and security layers.

For example:
we get incident tickets like
üëâ Capacity issues ‚Äì
We frequently handle OOM kills, high CPU usage, and disk full alerts in Kubernetes.
When this happens, we analyze pod metrics in Prometheus, check logs, resize resources, and  HPA tuning,scale nodes or pods to stabilize the system.

üëâ Cloud outages ‚Äì
We have managed AZ failures and DNS issues (Route53/CoreDNS).
In such cases, we perform traffic failover, update routing policies, and validate health checks to restore availability.

üëâ Bad deployments ‚Äì
Sometimes a canary or production release causes high 5xx errors or latency.
We immediately rollback, disable feature flags, analyze logs, fix the root cause, and redeploy safely.

üëâ Traffic spikes (Thundering Herd) ‚Äì
We‚Äôve handled sudden surges that overwhelmed databases and caches.
We implemented rate limiting, connection pooling, and cache optimization to stabilize traffic.

üëâ Application failures ‚Äì
We troubleshoot cascading failures between microservices.
We tune timeouts, add circuit breakers, and improve service dependencies.

üëâ Security incidents ‚Äì
We mitigate DDoS attacks and WAF blocks.
We also fix expired SSL certificates and rotated secrets breaking service communication.



‚ÄúOur main focus is quick detection, impact reduction, root cause analysis, and preventive measures.‚Äù
__________________________________________________
Daily roles

I check monitoring dashboards in Prometheus and Grafana to make sure all services are healthy.
I review any alerts from PagerDuty and fix critical issues.

if there is an incident, I investigate logs and metrics.
For example, if pods get OOMKilled, I tune HPA, resize resources, and scale nodes or pods.

during deployments, I monitor canary releases.
If I see 5xx errors, I rollback quickly and work with developers to fix the issue.

I automate repetitive tasks using scripts and Terraform.

after every major incident, I perform Root Cause Analysis (RCA) ‚Äì
I document what happened, why it happened, impact, action items, and preventive measures.

I review performance, capacity, and cost to keep systems optimized.

I handle security tasks like secret rotation, RBAC reviews, and WAF monitoring for  protects web applications by filtering malicious traffic and preventing common attacks.

Finally, I  update runbooks with troubleshooting steps, alert handling procedures, rollback processes, RCA templates, and lessons learned from past incidents.‚Äù
_____________--
RCA template---- for ref:
1) Incident Summary

What happened?

Date & time

Affected services

Severity (P1/P2)

2) Impact

Users impacted

Downtime duration

Business impact

3) Timeline

Detection time

Alert triggered

Actions taken

Resolution time

4) Root Cause

Technical reason

Why it happened

Contributing factors

5) Resolution

How we fixed it

Rollback / patch details

6) Preventive Actions

Monitoring improvements

Automation

Process changes

7) Lessons Learned

What went well

What to improve

8) Owners & Deadlines

Action items

Responsible person

ETA
_____________________________________________________________-
Interview one-liner

‚ÄúAn RCA template is a structured document we use after incidents to capture root cause, impact, resolution, and preventive actions.‚Äù
________________________________________________________________________














____________________________________________--
(this is little more expla---not req)
1.What type of incidents we manage.?

1) Capacity & Infrastructure Incidents

OOM & Resource Exhaustion

Pods getting OOMKilled, CPU throttling, disk reaching 100%.

Actions: Right-sizing resources, HPA tuning, node scaling, log cleanup, PVC expansion.

Cloud & Network Outages

Availability Zone failures, ELB health issues, DNS problems (Route53/CoreDNS).

Actions: Traffic failover, updating health checks, validating routing policies.

2) Deployment & Traffic Incidents

Bad Releases

Canary or production deployments causing 5xx errors, latency spikes.

Actions: Rollbacks, feature flag disable, root cause analysis.

Thundering Herd Problem

Sudden spike in DB connections or cache misses overwhelming backend.

Actions: Rate limiting, connection pooling, cache warming.

3) Application Performance Issues

Cascading Failures

One microservice failure impacting multiple downstream services.

Actions: Circuit breakers, timeout tuning, bulkhead pattern.

Database Locks & Thread Starvation

Row locks, thread pool exhaustion blocking transactions.

Actions: Query optimization, index tuning, increasing pool limits.

4) Security & Connectivity Incidents

Traffic Anomalies / DDoS

Sudden malicious traffic spikes.

Actions: WAF rules, IP blocking, autoscaling.

Credential & Certificate Failures

Expired SSL certs, rotated secrets breaking communication.

Actions: Secret rotation, cert automation (ACM/Cert-Manager).
