## Kubernetes ‚Äì Final Interview Revision Notes (L2 / Mid-Senior)

---

## 1. Pod

* Smallest deployable unit in Kubernetes
* Contains one or more containers sharing:

  * Network namespace (same IP)
  * Storage volumes
* Defined using a YAML manifest

### Static Pod

* Managed directly by **kubelet**, not by API Server
* Runs only on the **node where the manifest exists**
* Not scheduled by Kubernetes scheduler
* Used for control plane components

---

## 2. Services & Ingress

### Service (Expose Applications Internally / Externally)

Types:

* **ClusterIP** ‚Äì Default, internal-only access
* **NodePort** ‚Äì Exposes service on node IP and static port
* **LoadBalancer** ‚Äì Cloud-provider managed external LB

> ‚ö†Ô∏è Ingress is **NOT a Service type**

### Ingress

* Layer 7 (HTTP/HTTPS) routing
* Works **on top of Services**
* Provides path-based and host-based routing

---

## 3. ReplicationController (Deprecated)

* Maintains desired number of pod replicas
* Supports **only equality-based selectors**
* Replaced by **ReplicaSet**

---

## 4. ReplicaSet

* Ensures desired number of pods are running
* Supports:

  * Equality-based selectors
  * Set-based selectors (In, NotIn, Exists)
* Used internally by Deployments

---

## 5. Deployment

* Manages ReplicaSets
* Supports:

  * Rolling updates
  * Rollbacks
* Common update strategy: RollingUpdate

Key parameters:

* maxSurge
* maxUnavailable

---

## 6. DaemonSet

* Ensures **one pod per node**
* Automatically schedules pods on new nodes
* Common use cases:

  * Logging agents
  * Monitoring agents
  * Networking components

---

## 7. Horizontal Pod Autoscaler (HPA)

* Automatically scales pods based on metrics
* Metrics:

  * CPU
  * Memory
  * Custom metrics

Requirements:

* Metrics Server / Prometheus Adapter

---

## 8. Labels & Selectors

* Used to group and identify Kubernetes objects
* Services, ReplicaSets, and Deployments use selectors
* Do **NOT** perform scaling by themselves

---

## 9. Resource Requests & Limits

* **Requests** ‚Üí used by scheduler
* **Limits** ‚Üí enforced by kubelet

Rules:

* Request ‚â§ Limit
* CPU is throttled, memory is OOM-killed if exceeded

---

## 10. ConfigMap & Secret (MUST KNOW)

### ConfigMap

* Stores non-sensitive configuration
* Mounted as:

  * Environment variables
  * Files in a volume

### Secret

* Stores sensitive data (base64 encoded)
* Mounted same ways as ConfigMap

---

## 11. Storage Concepts

### PersistentVolume (PV)

* Cluster-level storage resource

### PersistentVolumeClaim (PVC)

* Request for storage by a pod

Rules:

* PVC binds to a single PV
* Matching based on:

  * Access mode
  * Storage size

### Dynamic Provisioning

* Uses StorageClass
* PV created automatically

---

## 12. StatefulSet (VERY IMPORTANT)

* Used for **stateful applications (Databases)**
* Features:

  * Stable pod identity
  * Stable storage
  * Ordered deployment & termination

Examples:

* MySQL, PostgreSQL, MongoDB

---

## 13. Liveness & Readiness Probes

### Liveness Probe

* Checks if container is alive
* Failing probe ‚Üí container restarted

### Readiness Probe

* Checks if container is ready to receive traffic
* Failing probe ‚Üí removed from Service endpoints

---

## 14. Scheduling Basics

### NodeSelector

* Simple key-value scheduling
* Limited functionality

### Node Affinity

* Advanced scheduling based on node labels
* Types:

  * requiredDuringSchedulingIgnoredDuringExecution (Hard)
  * preferredDuringSchedulingIgnoredDuringExecution (Soft)

---

## 15. Pod Affinity & Anti-Affinity

* Pod affinity matches **pod labels**, not node labels
* Used to place pods close to or away from each other

Topology Keys:

* kubernetes.io/hostname ‚Üí same node
* topology.kubernetes.io/zone ‚Üí same AZ

---

## 16. Taints & Tolerations

### Taint (Node)

* Prevents pods from scheduling

Command:

```
kubectl taint nodes <node-name> key=value:NoSchedule
```

### Toleration (Pod)

* Allows pod to tolerate a taint

Rule:

* Toleration must match taint key, value, and effect

---

## 17. Node Maintenance

### Cordon

* Mark node unschedulable

### Drain

* Evict pods safely
* Honors PodDisruptionBudgets

### Uncordon

* Enable scheduling again

---

## 18. PodDisruptionBudget (PDB)

* Ensures minimum availability during disruptions
* Used with drain and autoscaling

---

## 19. Init Containers

* Run before main containers
* Used for:

  * DB checks
  * Configuration setup

---

## 20. Namespaces

* Logical isolation of resources
* Used for:

  * Multi-team environments
  * Resource quotas

---

## 21. Common Troubleshooting

* Pod Pending ‚Üí Check node resources / affinity
* CrashLoopBackOff ‚Üí Check logs & probes
* ImagePullBackOff ‚Üí Check image name / registry access

Useful commands:

```
kubectl describe pod
kubectl logs pod-name
kubectl get events
```

---
##22.nodes-------cluster autoscaling:
EKS Cluster Auto scaler
======================
definition:
Even though we define CPU and memory requests and limits for pods, Kubernetes can schedule a pod only if there is enough capacity on existing worker nodes.
If all nodes are full, the pod goes into a pending state.
Cluster Autoscaler solves this by automatically adding new worker nodes so that pending pods can be scheduled.
Similarly, when nodes become underutilized, it removes them to optimize cost.‚Äù

The Cluster Autoscaler in Amazon EKS (Elastic Kubernetes Service) automatically adjusts the size of your EKS cluster based on the demands of your workloads. 
It helps manage the scaling of worker nodes in your cluster, adding nodes when there are pending pods that cannot be scheduled due to insufficient resources and 
removing nodes when they are underutilized.

practical way:
‚ÄúIn our EKS production cluster, we enabled Cluster Autoscaler with managed node groups.
During peak traffic, new pods became pending, and Cluster Autoscaler automatically added EC2 worker nodes.
After traffic reduced, unused nodes were terminated to reduce AWS cost.‚Äù

‚ÄúCluster Autoscaler watches pending pods and talks to AWS ASG to scale nodes.‚Äù
___________________________________________________________________

___________________________________________________________________
‚ÄúWhen we create an EKS cluster or node group, min and max values are configured only as boundaries for scaling.
Kubernetes itself does not increase or decrease nodes based on pod demand.
Without Cluster Autoscaler, the cluster will always stay at the desired node count, and pods will remain in a pending state if resources are insufficient.
Cluster Autoscaler is required to actively monitor pending pods and instruct the Auto Scaling Group to add or remove nodes within the defined min and max limits.‚Äù

Min = 2, Max = 4
Running nodes = 2

Pods increase üìà
‚Üì
Nodes full ‚ùå
‚Üì
Pods Pending ‚è≥
‚Üì
No Cluster Autoscaler ‚ùå
‚Üì
No new nodes added


Cluster Autoscaler scales ONLY IF:

Pods are Pending

Reason is Insufficient CPU/Memory

Pod can run on a new node

üö´ What Cluster Autoscaler Does NOT Do

‚ùå Does not scale pods (HPA does)

‚ùå Does not work without ASG / Managed Node Group

‚ùå Does not scale instantly (takes a few minutes)
___________________
what if max limit is reached?
we continuously monitor node, pod, and cluster-level
metrics to identify capacity issues and decide when to increase the maximum node limit or apply alternate scaling strategies.‚Äù


‚ÄúWhile continuously monitoring the cluster, if traffic increases and the max node limit is reached, we proactively increase the maximum node limit so Cluster Autoscaler can scale further.
Additionally, we handle extreme spikes using alternate methods like HPA tuning, pod priority & preemption, resource optimization, and larger instance types to ensure critical workloads
are scheduled.‚Äù
eksctl scale nodegroup \
  --cluster my-cluster \
  --name my-nodegroup \
  --nodes-min 2 \
  --nodes-max 8

____________________________________________________________________________________________________________________________________________





## FINAL INTERVIEW ONE-LINERS

* Deployment manages ReplicaSets
* StatefulSet is preferred for databases
* Requests affect scheduling, limits affect execution
* Node affinity attracts pods, taints repel pods
* Liveness restarts containers, readiness controls traffic

---

‚úÖ This document is sufficient for **final interview revision**.
