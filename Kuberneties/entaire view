1.POD------yaml file
2.static pod---deploy on same node
Static Pods are managed directly by kubelet and always run on the same node where their manifest exists (not managed by API server).

üëâ Interviewers expect: ‚Äúnot controlled by scheduler‚Äù
____________________________________________________
2.services-------to expose application-------i.Node port, ii.cluster ip, iii.LB, iv.Ingress
Correction (Very Important):

‚ùå Ingress is NOT a Service type

‚úÖ Ingress is a separate resource that works on top of Services

‚úî Correct version:

Services: ClusterIP, NodePort, LoadBalancer
Ingress: HTTP/HTTPS routing layer on top of Services
________________________________________________________________________________________________

3.Replication controller-----only equality based selector
_______________________________
4.Replica set----both equality & set based selector-----match labels, match expressions:key,,value,operator

___________________________________

5.Deployments------rollback  strategy
Deployments manage ReplicaSets and support rolling updates and rollbacks.
__________________________________________-
6.Deamon set---------deploy pod in all nodes-------no of nodes=no of pods deploy i.e pod deploy on each node 
DaemonSets ignore normal scheduling rules and do not require replicas.
__________________________________________________________________________________
7.HPA----mini replicas, max replicas-----based on metrics--cpu, memory
____________________________________________________________________________________
8.labels & selectors----------Labels and selectors are used to group and identify resources (Pods, Services, ReplicaSets).


üîß Correction:

Labels/selectors do not scale automatically

Scaling is done by HPA or controller


_______________________________________
9.Request, Limit----------Request should be less than limit------limit is high, that should be match b/w replicas has mentioned you

____________________________________________________________________________
10.DB-------PVC-----------deployment------env, volume mounts-name,path, volumes---name, pvcname
pvc--------selected/matched based on access modes & capacity of storage
Databases are recommended to use StatefulSets, not Deployments.
DB + PVC + Deployment
pvc------------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: app-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: gp3


_______________________________________________________________________________________________________
11.Dynamic volumes--------storage class based onthat------pv will create automatically with matched pvs
pv & pvc----------one-one relation ship
A PVC binds to one PV, but a PV can be reused after release (depending on reclaim policy).
___________________________________________________________________________
12.Livesness -------http:, path, entrypoint, port, initial delay sec, perdioc seconds, failure threshold

  Readiness probes
Liveness restarts containers, Readiness controls traffic flow.
  _____________________________________________________________________________________________________________
  13.Node selector----basic scheduling-------NodeSelector is simple key-value matching and is limited compared to Node Affinity
  i.node selector:
     disk=ssd

  __________________________
  13.ii.node affinity-------advanced scheduling:
     selectors:
       node affinity:-------preferred---check if not go with other option, requires-compulsary
       ____________
        affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: region
            operator: In
            values:
            - ap-south-1
          - key: zone
            operator: In
            values:
            - ap-south-1c
            ____________________------------
Node affinity ensures that pods are scheduled on specific nodes based on node labels.

Node affinity uses node labels?
‚úÖ YES
_________________________________________________
scheduler checks all nodes

Filters nodes with label env=prod

Picks the best node based on resources

Schedules the pod

If no node matches ‚Üí pod Pending

___________________________________________________________________________________________________________-

13.iii.----pod Affinity:
 affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - frontend
        topologyKey: "kubernetes.io/hostname
__________________________________________________________--
| topologyKey                   | Meaning     |
| ----------------------------- | ----------- |
| kubernetes.io/hostname        | Same node   |
| topology.kubernetes.io/zone   | Same AZ     |
| topology.kubernetes.io/region | Same region |

        ________________________________________________________________________
        13.iv----pod antiaffinity:
        affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:                  #Preferred(soft rule)
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - frontend
        topologyKey: "kubernetes.io/hostname"
__________________________________________________________________
vvimp:
Pod affinity allows a pod to be scheduled near another pod based on pod labels and topology keys.

Scheduler looks for backend pods

Checks their labels

Finds the node where backend pod is running

Places frontend pod on the same topologyKey

If not possible ‚Üí pod stays Pending
_________________________________________________________________________
| Question                           | Answer |
| ---------------------------------- | ------ |
| Pod affinity matches node labels?  | ‚ùå NO   |
| Pod affinity matches pod labels?   | ‚úÖ YES  |
| Node affinity matches node labels? | ‚úÖ YES  |




-----------------------------------------         _________________________________
14.taints & tolerations
Taints--------applicable to only nodes
kubectl taint nodes <node-name> key=value:NoSchedule


Tolerations--applicable to Pod---write under spec
key:
value:
operator:
effect:
___________________________________________________________________________________________________
Taints , Tolerations vs Node Affinity:Differences
1.Node affinity is a pod‚Äôs preference to choose nodes, whereas taints and tolerations are a node‚Äôs way of rejecting pods.
Node Affinity (Pod ‚Üí Node)
What it does

Pod selects which nodes it wants

Based on node labels
________________________
2.Taints & Tolerations (Node ‚Üí Pod)

Node affinity is like a pod requesting a specific node,
whereas taints and tolerations are like a node allowing or denying pods based on taints.
Tolerations on the pod must match the node‚Äôs taints for the pod to be scheduled.

Node affinity is pod-driven scheduling, while taints and tolerations are node-driven access control.
________________________-
What it does

Node repels pods

Only pods with matching tolerations can run

Example use case

Reserve nodes only for database or critical workloads
| Concept       | Easy Meaning                  |
| ------------- | ----------------------------- |
| Node Affinity | Pod says: *I want this node*  |
| Taint         | Node says: *You cannot come*  |
| Toleration    | Pod says: *I have permission* |

__________________________________________________________________________----------------

15.cardon, drain, uncardon:
1. **Cordon** ‚Äî Stop scheduling new Pods on Node (existing Pods stay running).
2. **Drain** ‚Äî Move all Pods off Node (eviction) + mark Node unschedulable.
3. **Uncordon** ‚Äî Allow scheduling again (Node back to normal).

Cardon:
kubectl cordon <node-name>

exmp:
kubectl cordon ip-192-168-59-88.ap-south-1.compute.internal

* Node status changes to `SchedulingDisabled`.
* Running Pods stay untouched.
* New Pods are prevented from scheduling here.
____________________________________
Drain ‚Äî Evict All Pods from Node (Safe Removal)

kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data

exmp:
kubectl drain ip-192-168-59-88.ap-south-1.compute.internal --ignore-daemonsets --delete-emptydir-data

 Kubernetes **evicts all regular Pods** safely.
* Pod eviction honors PodDisruptionBudgets (if configured).
* DaemonSet Pods stay.
* Node remains in `SchedulingDisabled` state after draining.
__________________________________________________
3.uncordon:Make Node available again for new Pods to be scheduled.
kubectl uncordon <node-name>
Pods can now be scheduled onto this Node again.




## Real-World Use Cases:

| Scenario                            | Solution                                     |
| ----------------------------------- | -------------------------------------------- |
| Upgrade Kubernetes or Node OS       | cordon ‚Üí drain ‚Üí patch ‚Üí uncordon            |
| Move workloads from old Node        | cordon ‚Üí drain ‚Üí delete Node                 |
| Node disk issue (temporary removal) | cordon ‚Üí drain ‚Üí fix ‚Üí uncordon              |
| Debug performance issues on Node    | cordon ‚Üí drain (optional) ‚Üí debug ‚Üí uncordon |

---

## Command Cheat Sheet:

| Action                       | Command Example                                                      |
| ---------------------------- | -------------------------------------------------------------------- |
| Mark Node Unschedulable      | kubectl cordon <node-name>                                           |
| Evict Pods and Unschedulable | kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data |
| Allow Node Scheduling Again  | kubectl uncordon <node-name>                                         |
| Check Node Status            | kubectl get nodes                                                    |
___________________________________________________________________________-
RBAC

__________________________________________________________________________________________
RPOL





