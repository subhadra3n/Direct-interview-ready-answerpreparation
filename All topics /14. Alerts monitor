How we managed the incidents
In production, we receive alerts through Slack or PagerDuty configured from Prometheus Alertmanager.
The alert contains pod name and namespace.

After receiving the alert, I log into the bastion server, check the pod status using kubectl, describe the pod to see events, and analyze container logs.

Based on logs, I identify root cause and apply fix.

__________________________________________________

In production, we faced pod crashes due to config issues, node failures, high memory usage causing OOMKilled, ingress misconfigurations,
and image pull failures.
We resolved them by checking logs, scaling pods, fixing configs, draining nodes, and rolling back deployments.

Kubernetes Production Incidents
1Ô∏è‚É£ Application Pod Crash (CrashLoopBackOff)

App crashes due to:

Wrong env variables

DB connection failure

Code bug

Impact: Service unavailable

2Ô∏è‚É£ Node NotReady

Node goes down due to:

Disk full

Network issue

Kernel panic

Impact: Pods evicted

3Ô∏è‚É£ High CPU / Memory usage

Sudden traffic spike

Memory leak in app

Impact:

Pod throttling

OOMKilled

4Ô∏è‚É£ Image Pull Failure

Wrong image tag

DockerHub limit

Private registry auth issue

5Ô∏è‚É£ Service Down

Wrong selector

Port mismatch

Network policy blocking

6Ô∏è‚É£ ETCD latency / failure

Disk IO issue

Snapshot failure

Impact: Cluster unstable

7Ô∏è‚É£ Ingress not routing traffic

TLS cert expired

DNS issue

Wrong path rules

8Ô∏è‚É£ ConfigMap / Secret missing

App fails to start

CrashLoopBackOff

9Ô∏è‚É£ HPA not scaling

Metrics server down

Prometheus issue

üîü Persistent Volume full

Disk full

DB pods crash
