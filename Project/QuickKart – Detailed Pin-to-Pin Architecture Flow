QuickKart ‚Äì Detailed Pin-to-Pin Architecture Flow


1Ô∏è‚É£ VPC & Subnets Setup

VPC: Contains all networking resources (CIDR, route tables).

Public Subnets:

Application Load Balancer (ALB)

NAT Gateway

Private Subnets:

EKS worker nodes / pods

Databases (RDS/DynamoDB)

Key: Private subnets cannot access the internet directly; they route outbound traffic via NAT Gateway in a public subnet.

2Ô∏è‚É£ Application Placement

EKS Cluster: Deployed in private subnets for security.

Pods: Run in private subnets on worker nodes (EC2 or Fargate).

Services inside EKS:

Catalog Service Pod ‚Äì handles product CRUD

Notification Service Pod ‚Äì sends email/push notifications

Pod Type: Can use Deployment Pods for stateless services.

Enables auto-scaling with HPA (Horizontal Pod Autoscaler).

Rolling updates can be done without downtime.

3Ô∏è‚É£ Traffic Flow ‚Äì End to End
User ‚Üí Application
________________________________________________________
User Browser / Mobile App
        |
        v
Route 53 (DNS Resolution)
        |
        v
Application Load Balancer (ALB) ‚Äì Public Subnet
        |
        v
Kubernetes Ingress Controller (AWS Load Balancer Controller / NGINX)
        |
        v
Kubernetes Service
        |
        v
Application Pods ‚Äì EKS Cluster (Private Subnet)

Explanation

The user accesses QuickKart using a domain name.

Route 53 resolves the domain to the ALB DNS name.

ALB (Public Subnet) terminates HTTPS and routes traffic based on rules (path/host).
ALB handles HTTP/S routing and forwards traffic to the correct service pods.

Traffic is forwarded to the Kubernetes Ingress Controller.

Ingress routes traffic to the appropriate Kubernetes Service.

Service forwards traffic to EKS Pods running in Private Subnets.


______________________________________________________________
Pod ‚Üí Internet (Outbound)

Pod in Private Subnet
      |
      v
Route via Private Subnet Route Table
      |
      v
NAT Gateway (Public Subnet)
      |
      v
Internet

Example: Pods may pull Docker images from ECR or download external dependencies. NAT Gateway provides secure internet access for pods in private subnets.

Explanation

Pods may need outbound internet access for:

Pulling Docker images from Amazon ECR

Accessing third-party APIs

Downloading OS or application dependencies

Since pods are in private subnets, they cannot access the internet directly.

Traffic is routed via the NAT Gateway, which provides secure outbound access only.

No inbound internet access is allowed through NAT, ensuring security.


_________________________________________________________________________----
Pod ‚Üí Database (Internal Communication Flow)
Application Pod (Catalog / Orders / Notifications)
        |
        v
Database (RDS / DynamoDB) ‚Äì Private Subnet
Explanation

Application pods communicate with databases using private IPs.

Databases are deployed in private subnets.

No public endpoints for databases.

Communication stays within the VPC, protected by:

Security Groups

Network ACLs

IAM roles (for DynamoDB)

üëâ This ensures data security and compliance.
__________________________________
Databases are private, not directly exposed to the internet.

Communication is internal within the VPC.
CI/CD Flow (Code ‚Üí Production)
Developer
   |
   v
Git Repository
   |
   v
Jenkins (EC2 / Private Subnet)
   |
   |-- Build & Test
   |-- Docker Image Build
   |-- Push Image to ECR (Private + NAT Gateway)
   |
   v
Deploy to EKS (Helm / kubectl)

Explanation

Developer commits code to Git.

Jenkins triggers automatically.

Jenkins builds the application and creates a Docker image.

Image is pushed securely to Amazon ECR.

Deployment is performed using:

Helm charts

or kubectl

or Argo CD (GitOps) for auto-sync deployments.
____________________
Monitoring & Logging

Application Pods (Private Subnet)
        |
        v
CloudWatch Logs & Metrics
        |
        v
Prometheus (Metrics Collection)
        |
        v
Grafana Dashboards (Visualization & Alerts)
Application and container logs are sent to CloudWatch.

Prometheus scrapes metrics from pods and nodes.

Grafana visualizes:

CPU / Memory usage

Pod health

Request latency

Error rates

Alerts can be integrated with:

SNS

Email

PagerDuty
______________________
Monitoring is centralized and secured within the VPC.

4.Pod & Scaling Details
| Component            | Pod Type         | Scaling          | Notes                                     |
| -------------------- | ---------------- | ---------------- | ----------------------------------------- |
| Catalog Service      | Deployment       | HPA (CPU/Memory) | Stateless, can scale horizontally         |
| Notification Service | Deployment       | HPA (CPU/Memory) | Stateless, handles synchronous REST calls |
| Jenkins Agent Pods   | Deployment / Job | Dynamic          | Temporary pods for CI/CD builds           |
| Other microservices  | Deployment       | HPA / Manual     | Stateless, independently deployable       |
5Ô∏è‚É£ Pin-to-Pin Network Summary

Inbound: Users ‚Üí Route 53 ‚Üí ALB ‚Üí Kubernetes Ingress ‚Üí Service Pods (private subnet)

Internal: Pods ‚Üí Databases (private subnet)

Outbound: Pods ‚Üí NAT Gateway (public subnet) ‚Üí Internet (ECR, APIs, updates)

CI/CD: Jenkins builds ‚Üí Docker images ‚Üí ECR ‚Üí Deploy ‚Üí Pods (private subnet)

Scaling: HPA monitors pod metrics ‚Üí increases/decreases replicas in private subnets

Monitoring: Pods ‚Üí CloudWatch ‚Üí Prometheus/Grafana

‚úÖ Key Points to Emphasize in Interview

Application pods are in private subnets for security.

NAT Gateway allows controlled outbound internet access.

Deployment pods + HPA enable auto-scaling.

Stateless services make rolling updates smooth.

Databases are private, not internet-facing.

CI/CD is fully automated with Jenkins + ECR + Helm/Kubectl.



‚ÄúIn QuickKart, users access the application through Route 53 and ALB, traffic is routed via Kubernetes Ingress to EKS pods running in private subnets.
Outbound pod traffic goes through a NAT Gateway, databases remain private inside the VPC,
CI/CD is handled via Jenkins and ECR, and the entire platform is monitored using CloudWatch, Prometheus, and Grafana.‚Äù
